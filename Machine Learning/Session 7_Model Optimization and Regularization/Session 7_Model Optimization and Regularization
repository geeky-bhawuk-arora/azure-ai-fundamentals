{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":["BL4FBftoftog"],"authorship_tag":"ABX9TyMyNv2H0h1+REuWk+741s0O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Parametric and Nonparametric Machine Learning Algorithms\n","\n","### Learning a Function\n","\n","Machine learning can be summarized as learning a function (f) that maps input variables (X) to output variables (Y).\n","\n","Y = f(x)\n","\n","An algorithm learns this target mapping function from training data.\n","\n","The form of the function is unknown, so our job as machine learning practitioners is to evaluate different machine learning algorithms and see which is better at approximating the underlying function.\n","\n","Different algorithms make different assumptions or biases about the form of the function and how it can be learned.\n","\n","\n","## Parametric Machine Learning Algorithms\n","\n","Assumptions can greatly simplify the learning process, but can also limit what can be learned. Algorithms that simplify the function to a known form are called parametric machine learning algorithms.\n","\n","- A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model. No matter how much data you throw at a parametric model, it won’t change its mind about how many parameters it needs.\n","\n","\n","The algorithms involve two steps:\n","\n","1. Select a form for the function.\n","2. Learn the coefficients for the function from the training data.\n","\n","An easy to understand functional form for the mapping function is a line, as is used in linear regression:\n","\n","$b_0 + b_1*x_1 + b_2*x_2 = 0$\n","\n","Where $b_0$, $b_1$ and $b_2$ are the coefficients of the line that control the intercept and slope, and $x_1$ and $x_2$ are two input variables.\n","\n","Assuming the functional form of a line greatly simplifies the learning process. Now, all we need to do is estimate the coefficients of the line equation and we have a predictive model for the problem.\n","\n","Often the assumed functional form is a linear combination of the input variables and as such parametric machine learning algorithms are often also called “linear machine learning algorithms“.\n","\n","The problem is, the actual unknown underlying function may not be a linear function like a line. It could be almost a line and require some minor transformation of the input data to work right. Or it could be nothing like a line in which case the assumption is wrong and the approach will produce poor results.\n","\n","Some more examples of parametric machine learning algorithms include:\n","\n","* Logistic Regression\n","* Linear Discriminant Analysis\n","* Perceptron\n","* Naive Bayes\n","* Simple Neural Networks\n","\n","**Benefits of Parametric Machine Learning Algorithms:**\n","\n","* Simpler: These methods are easier to understand and interpret results.\n","* Speed: Parametric models are very fast to learn from data.\n","* Less Data: They do not require as much training data and can work well even if the fit to the data is not perfect.\n","\n","**Limitations of Parametric Machine Learning Algorithms:**\n","\n","- Constrained: By choosing a functional form these methods are highly constrained to the specified form.\n","- Limited Complexity: The methods are more suited to simpler problems.\n","- Poor Fit: In practice the methods are unlikely to match the underlying mapping function.\n","\n","**Nonparametric Machine Learning Algorithms**\n","\n","Algorithms that do not make strong assumptions about the form of the mapping function are called nonparametric machine learning algorithms. By not making assumptions, they are free to learn any functional form from the training data.\n","\n","- Nonparametric methods are good when you have a lot of data and no prior knowledge, and when you don’t want to worry too much about choosing just the right features.\n","\n","\n","\n","Nonparametric methods seek to best fit the training data in constructing the mapping function, whilst maintaining some ability to generalize to unseen data. As such, they are able to fit a large number of functional forms.\n","\n","An easy to understand nonparametric model is the k-nearest neighbors algorithm that makes predictions based on the k most similar training patterns for a new data instance. The method does not assume anything about the form of the mapping function other than patterns that are close are likely to have a similar output variable.\n","\n","Some more examples of popular nonparametric machine learning algorithms are:\n","\n","- k-Nearest Neighbors\n","- Decision Trees like CART and C4.5\n","- Support Vector Machines\n","\n","**Benefits of Nonparametric Machine Learning Algorithms:**\n","\n","- Flexibility: Capable of fitting a large number of functional forms.\n","- Power: No assumptions (or weak assumptions) about the underlying function.\n","- Performance: Can result in higher performance models for prediction.\n","\n","**Limitations of Nonparametric Machine Learning Algorithms:**\n","\n","- More data: Require a lot more training data to estimate the mapping function.\n","- Slower: A lot slower to train as they often have far more parameters to train.\n","- Overfitting: More of a risk to overfit the training data and it is harder to explain why specific predictions are made.\n"],"metadata":{"id":"RXiIjfKoOJYy"}},{"cell_type":"markdown","source":["# Hyperparameter Tuning\n","\n","First, let’s understand the differences between a hyperparameter and a parameter in machine learning.\n","\n","- **Model parameters:** These are the parameters that are estimated by the model from the given data. For example the weights of a deep neural network. \n","- **Model hyperparameters:** These are the parameters that cannot be estimated by the model from the given data. These parameters are used to estimate the model parameters. For example, the learning rate in deep neural networks.\n","\n","![](https://i.imgur.com/oEdA45H.jpg)\n","\n","\n","**Hyperparameter tuning (or hyperparameter optimization)** is the process of determining the right combination of hyperparameters that maximizes the model performance. \n","\n","It works by running multiple trials in a single training process. Each trial is a complete execution of your training application with values for your chosen hyperparameters, set within the limits you specify. \n","\n","This process once finished will give you the set of hyperparameter values that are best suited for the model to give optimal results.  \n","\n"],"metadata":{"id":"PxU7iyxpQdsV"}},{"cell_type":"markdown","source":["# Cross Validation\n","Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. Use cross-validation to detect overfitting, ie, failing to generalize a pattern."],"metadata":{"id":"0syQrPx08abk"}},{"cell_type":"markdown","source":["## K-fold Cross Validation\n","\n","Let’s say that you have trained a machine learning model. Now, you need to find out how well this model performs. Is it accurate enough to be used? How does it compare to another model? There are several evaluation methods to determine this. One such method is called K-fold cross validation.\n","![](https://i.imgur.com/NJkv5Tk.jpg)\n","\n","Cross validation is an evaluation method used in machine learning to find out how well your machine learning model can predict the outcome of unseen data. It is a method that is easy to comprehend, works well for a limited data sample and also offers an evaluation that is less biased, making it a popular choice.\n","\n","The data sample is split into **‘k’ number of smaller samples**, hence the name: K-fold Cross Validation. You may also hear terms like four fold cross validation, or ten fold cross validation, which essentially means that the sample data is being split into four or ten smaller samples respectively.\n","\n","\n","## How is k-fold cross validation performed?\n","\n","The general stratergy is quite straight forward and the following steps can be used:\n","\n","- First, shuffle the dataset and split into k number of subsamples. (It is important to try to make the subsamples equal in size and ensure k is less than or equal to the number of elements in the dataset).\n","- In the first iteration, the first subset is used as the test data while all the other subsets are considered as the training data.\n"," -Train the model with the training data and evaluate it using the test subset. Keep the evaluation score or error rate, and get rid of the model.\n","- Now, in the next iteration, select a different subset as the test data set, and make everything else (including the test set we used in the previous iteration) part of the training data.\n","- Re-train the model with the training data and test it using the new test data set, keep the evaluation score and discard the model.\n","- Continue iterating the above k times. Each data subsamples will be used in each iteration until all data is considered. You will end up with a k number of evaluation scores.\n","- The total error rate is the average of all these individual evaluation scores.\n","\n","![](https://i.imgur.com/6aZw9So.png)\n","\n","\n","## How to determine the best value for ‘k’ in K-Fold Cross Validation?\n","\n","Chosing a good value for k is important. A poor value for k can result in a poor evaluation of the model’s abilities. In other words, it can cause the measured ability of the model to be overestimated (high bias) or change widely depending on the training data used (high variance).\n","\n","Generally, there are three ways to select k:\n","\n","1. Let k = 5, or k =10. Through experimentation, it has been found that selecting k to be 5 or 10 results in sufficiently good results.\n","2. Let k = n, where n is the size of the dataset. This ensures each sample is used in the test data set.\n","3. Another way is to chose k so that every split data sample is sufficiently large, ensuring they are statistically represented in the larger dataset.\n","\n","## Types of cross validation\n","\n","Cross validation can be divided into two major categories:\n","\n","1. **Exhaustive**, where the method learn and test on every single possibility of dividing the dataset into training and testing subsets.\n","2. **Non-exhaustive cross validation** methods where all ways of splitting the sample are not computed.\n","\n","**Exhaustive cross-validation**\n","\n","**Leave-p-out cross validation** is a method of exhaustive cross validation. Here, p number of observations (or elements in the sample dataset) are left out as the training dataset, everything else is considered as part of the training data. For more clarity, if you look at the above image, p is equal to 5, as shown by the 5 circles in the ‘test data’.\n","\n","**Leave-one-out cross validation** a special form of leave-p-out exhuastive cross validation method, where p = 1. This is also a specific case for k-fold cross validation, where k = N(number of elements in the sample dataset).\n","\n","***Mathematical Expression*** \n","\n","LOOCV involves one fold per observation i.e each observation by itself plays the role of the validation set. The (N-1) observations play the role of the training set. With least-squares linear, a single model performance cost is the same as a single model. In LOOCV, refitting of the model can be avoided while implementing the LOOCV method. MSE(Mean squared error) is calculated by fitting on the complete dataset.\n","\n","![](https://i.imgur.com/b6zqrU4.png)\n","\n","In the above formula, $h_i$ represents how much influence an observation has on its own fit i.e between 0 and 1 that punishes the residual, as it divides by a small number. It inflates the residual.\n","\n","\n","**Non-exhaustive cross-validation**\n","\n","K-fold cross validation where k is not equal to N, Stratified cross validation and repeated random sub-sampling validation are non-exhaustive cross validation methods.\n","\n","**Stratified cross validation:** There can be some tricky situation with K Fold validation.\n","\n","Since we are randomly shuffling the data and dividing it into folds. Chances are we may get highly imbalanced folds which can cause our training to be biased.\n","\n","![](https://i.imgur.com/96J2TrZ.jpg)\n","\n","Let say, somehow we get a fold that has majority belongs to one class (say dog class) and only a few as cat class. This will certainly create problem in our training and to avoid this we make stratified folds using stratification.\n","\n","**Stratification** is a process of rearranging the data to ensure that each fold or group is a good representative of the complete dataset.\n","\n","For example, in binary classification, every split has elements of which roughly 50% belongs to class 0 and 50% that belongs to class 1.\n","\n","\n","Partitions are selected such that each partition contains roughly the same amount of elements for each class label. \n","\n","**Repeated random sub-sampling validation (Monte Carlo cross validation):** Data is split into multiple random subsets and the model is trained and evaluated for each split. The results are averaged over the splits. Unlike the k-fold cross validation, proportions of the training and test set size are not dependent on the size of the data set, which is an advantage. However, a disadvantage is that some data elements will never be selected as a part of the test set, while some may be selected multiple times. When the amount of random splits are increased and approach infinity, the results tend to be similar to that of leave-p-out cross validation.\n","\n","**Steps:**\n","\n","- Split training data randomly (maybe 70–30% split or 62.5–37.5% split or 86.3–13.7%split). For each iteration, the train-test split percentage is different.\n","- Fit the model on train data set for that iteration and calculate test error using the fitted model on test data\n","- Repeat many iterations (say 100 or 500 or even 1000 iterations) and take the average of the test errors.\n","\n","![](https://i.imgur.com/Raz2ovj.png)\n","\n","**Note - the same data can be selected more than once in the test set or even never at all.**"],"metadata":{"id":"whGlPC9_WlEm"}},{"cell_type":"markdown","source":["### Cross Validation Practical Implimentation"],"metadata":{"id":"M5OADdLHY9ME"}},{"cell_type":"code","source":["from numpy import array\n","from sklearn.model_selection import KFold\n","\n","# data sample\n","data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n","data\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b0n0TzDcBZI3","executionInfo":{"status":"ok","timestamp":1669199214397,"user_tz":-330,"elapsed":1495,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"cc8dd60c-00bb-4f68-df23-e31171fe8255"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["# prepare cross validation\n","kfold = KFold(n_splits = 3, shuffle = True, random_state = 1)\n","# enumerate splits\n","for train, test in kfold.split(data):\n","    print('train: %s, test: %s' % (data[train], data[test]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"36seiHLZBj7W","executionInfo":{"status":"ok","timestamp":1669199219088,"user_tz":-330,"elapsed":5,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"a5af7ce0-000d-41c3-81ba-fdb472135598"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train: [0.1 0.4 0.5 0.6], test: [0.2 0.3]\n","train: [0.2 0.3 0.4 0.6], test: [0.1 0.5]\n","train: [0.1 0.2 0.3 0.5], test: [0.4 0.6]\n"]}]},{"cell_type":"markdown","source":["### **Red Wine Quality Prediction with Cross Validation and Hyperparameter Tuning**\n","\n","![](https://i.imgur.com/JebHz69.png)\n","\n","This datasets is related to red variants of the Portuguese \"Vinho Verde\" wine. For more details, consult the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n","\n","The datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are much more normal wines than excellent or poor ones).\n","\n","This dataset is also available from the UCI machine learning repository, https://archive.ics.uci.edu/ml/datasets/wine+quality , I just shared it to kaggle for convenience. (If I am mistaken and the public license type disallowed me from doing so, I will take this down if requested.)\n","\n","Content\n","\n","For more information, read [Cortez et al., 2009].\n","**Input variables (based on physicochemical tests):**\n","1. fixed acidity\n","2. volatile acidity\n","3. citric acid\n","4. residual sugar\n","5. chlorides\n","6. free sulfur dioxide\n","7. total sulfur dioxide\n","8. density\n","9. pH\n","10. sulphates\n","11. alcohol\n","\n","**Output variable (based on sensory data):**\n","12. quality (score between 0 and 10)\n","\n","**Tips**\n","\n","What might be an interesting thing to do, is aside from using regression modelling, is to set an arbitrary cutoff for your dependent variable (wine quality) at e.g. 7 or higher getting classified as 'good/1' and the remainder as 'not good/0'. This allows you to practice with hyper parameter tuning on e.g. decision tree algorithms looking at the ROC curve and the AUC value. Without doing any kind of feature engineering or overfitting you should be able to get an AUC of .88 (without even using random forest algorithm)\n","\n","**Inspiration**\n","\n","Use machine learning to determine which physiochemical properties make a wine 'good'!\n","\n","**Acknowledgements**\n","\n","This dataset is also available from the UCI machine learning repository, https://archive.ics.uci.edu/ml/datasets/wine+quality , I just shared it to kaggle for convenience. (I am mistaken and the public license type disallowed me from doing so, I will take this down at first request. I am not the owner of this dataset.\n","\n","Please include this citation if you plan to use this database:\n","P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\n","\n","**Relevant publication**\n","\n","P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009"],"metadata":{"id":"dB9I-Nf_WVcj"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","url='https://raw.githubusercontent.com/ubaid-shah/datasets/main/winequality-red.csv'\n","df = pd.read_csv(url)\n","\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":267},"id":"MCr4RuNQVjOd","executionInfo":{"status":"ok","timestamp":1669540350192,"user_tz":-330,"elapsed":2154,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"7a97ecc9-b67c-4ff2-fbba-ef9764c17458"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n","0            7.4              0.70         0.00             1.9      0.076   \n","1            7.8              0.88         0.00             2.6      0.098   \n","2            7.8              0.76         0.04             2.3      0.092   \n","3           11.2              0.28         0.56             1.9      0.075   \n","4            7.4              0.70         0.00             1.9      0.076   \n","\n","   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n","0                 11.0                  34.0   0.9978  3.51       0.56   \n","1                 25.0                  67.0   0.9968  3.20       0.68   \n","2                 15.0                  54.0   0.9970  3.26       0.65   \n","3                 17.0                  60.0   0.9980  3.16       0.58   \n","4                 11.0                  34.0   0.9978  3.51       0.56   \n","\n","   alcohol  quality  \n","0      9.4        5  \n","1      9.8        5  \n","2      9.8        5  \n","3      9.8        6  \n","4      9.4        5  "],"text/html":["\n","  <div id=\"df-9e09e1e0-d8ed-4e6d-9dc7-866b37298c01\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fixed acidity</th>\n","      <th>volatile acidity</th>\n","      <th>citric acid</th>\n","      <th>residual sugar</th>\n","      <th>chlorides</th>\n","      <th>free sulfur dioxide</th>\n","      <th>total sulfur dioxide</th>\n","      <th>density</th>\n","      <th>pH</th>\n","      <th>sulphates</th>\n","      <th>alcohol</th>\n","      <th>quality</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7.4</td>\n","      <td>0.70</td>\n","      <td>0.00</td>\n","      <td>1.9</td>\n","      <td>0.076</td>\n","      <td>11.0</td>\n","      <td>34.0</td>\n","      <td>0.9978</td>\n","      <td>3.51</td>\n","      <td>0.56</td>\n","      <td>9.4</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7.8</td>\n","      <td>0.88</td>\n","      <td>0.00</td>\n","      <td>2.6</td>\n","      <td>0.098</td>\n","      <td>25.0</td>\n","      <td>67.0</td>\n","      <td>0.9968</td>\n","      <td>3.20</td>\n","      <td>0.68</td>\n","      <td>9.8</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>7.8</td>\n","      <td>0.76</td>\n","      <td>0.04</td>\n","      <td>2.3</td>\n","      <td>0.092</td>\n","      <td>15.0</td>\n","      <td>54.0</td>\n","      <td>0.9970</td>\n","      <td>3.26</td>\n","      <td>0.65</td>\n","      <td>9.8</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.2</td>\n","      <td>0.28</td>\n","      <td>0.56</td>\n","      <td>1.9</td>\n","      <td>0.075</td>\n","      <td>17.0</td>\n","      <td>60.0</td>\n","      <td>0.9980</td>\n","      <td>3.16</td>\n","      <td>0.58</td>\n","      <td>9.8</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7.4</td>\n","      <td>0.70</td>\n","      <td>0.00</td>\n","      <td>1.9</td>\n","      <td>0.076</td>\n","      <td>11.0</td>\n","      <td>34.0</td>\n","      <td>0.9978</td>\n","      <td>3.51</td>\n","      <td>0.56</td>\n","      <td>9.4</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e09e1e0-d8ed-4e6d-9dc7-866b37298c01')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9e09e1e0-d8ed-4e6d-9dc7-866b37298c01 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9e09e1e0-d8ed-4e6d-9dc7-866b37298c01');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xgF3caNKXN7q","executionInfo":{"status":"ok","timestamp":1669540418066,"user_tz":-330,"elapsed":7,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"52e91384-beb7-4a89-8d4b-3de1373a516c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fixed acidity           0\n","volatile acidity        0\n","citric acid             0\n","residual sugar          0\n","chlorides               0\n","free sulfur dioxide     0\n","total sulfur dioxide    0\n","density                 0\n","pH                      0\n","sulphates               0\n","alcohol                 0\n","quality                 0\n","dtype: int64"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["X = df.drop(columns='quality')\n","y = df.quality"],"metadata":{"id":"ZCV0DqDmW1hK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['quality'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"335NukIWXUOQ","executionInfo":{"status":"ok","timestamp":1669540445122,"user_tz":-330,"elapsed":572,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"1977b248-acc0-4833-b53c-bfca0f54ed94"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5    681\n","6    638\n","7    199\n","4     53\n","8     18\n","3     10\n","Name: quality, dtype: int64"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"],"metadata":{"id":"ksDzFj9CXbpo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","classifier = RandomForestClassifier(n_estimators=100, random_state=0)"],"metadata":{"id":"nzdCPdYZXdty"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There is another way to use K-fold cross validation using `cross_val_score`. Here we use `cv=number of k folds`"],"metadata":{"id":"CEvVJqQQXoEW"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","all_accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=5)"],"metadata":{"id":"k0bfnF_5Xhv8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(all_accuracies)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Qhim5naX6oy","executionInfo":{"status":"ok","timestamp":1669540598415,"user_tz":-330,"elapsed":7,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"4e12e192-9cf0-4304-87e7-98e2d70afcc0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.640625   0.62890625 0.671875   0.6640625  0.6745098 ]\n"]}]},{"cell_type":"code","source":["print(all_accuracies.mean())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ziXBdGriX_DJ","executionInfo":{"status":"ok","timestamp":1669540622471,"user_tz":-330,"elapsed":7,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"0e634257-b955-4c5a-af8e-7f58740bd84b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6559957107843137\n"]}]},{"cell_type":"markdown","source":["## Grid Search CV and Randomized Search CV\n","\n","When tuning the hyperparameters of an estimator, Grid Search and Random Search are both popular methods.\n","\n","**Grid Search CV**\n","\n","Grid Search can be thought of as an exhaustive search for selecting a model. In Grid Search, the data scientist sets up a grid of hyperparameter values and for each combination, trains a model and scores on the testing data. In this approach, every combination of hyperparameter values is tried which can be very inefficient. \n","![](https://i.imgur.com/kPqJi4f.jpg)\n","\n","For example, searching 20 different parameter values for each of 4 parameters will require 160,000 trials of cross-validation. This equates to 1,600,000 model fits and 1,600,000 predictions if 10-fold cross validation is used. \n","\n","**While Scikit Learn offers the GridSearchCV function to simplify the process, it would be an extremely costly execution both in computing power and time.**\n","\n","\n","**Randomized Search CV**\n","\n","By contrast, Random Search sets up a grid of hyperparameter values and selects random combinations to train the model and score. This allows you to explicitly control the number of parameter combinations that are attempted. The number of search iterations is set based on time or resources. Scikit Learn offers the RandomizedSearchCV function for this process.\n","![](https://i.imgur.com/khSOoko.jpg)\n","\n","While it’s possible that RandomizedSearchCV will not find as accurate of a result as GridSearchCV, it surprisingly picks the best result more often than not and in a fraction of the time it takes GridSearchCV would have taken. \n","\n","**Given the same resources, Randomized Search can even outperform Grid Search. This can be visualized in the graphic below when continuous parameters are used.**"],"metadata":{"id":"BL4FBftoftog"}},{"cell_type":"code","source":["# Sample code for grid search CV\n","\n","import itertools\n","\n","p1 = [\"A\", \"B\", \"C\", \"D\"]\n","p2 = [True, False]\n","\n","c = itertools.product(p1, p2)\n","for i in c:\n","    print(i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BvZsKB_wYeEJ","executionInfo":{"status":"ok","timestamp":1669540754757,"user_tz":-330,"elapsed":616,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"8da7a56c-168a-4626-bf2f-cb3a2780921f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('A', True)\n","('A', False)\n","('B', True)\n","('B', False)\n","('C', True)\n","('C', False)\n","('D', True)\n","('D', False)\n"]}]},{"cell_type":"markdown","source":["Random forest will have following hyper parameters\n","\n","```\n","RandomForestClassifier**(\n","    n_estimators=100,\n","     criterion='gini',\n","    max_depth=None,\n","    min_samples_split=2,\n","    min_samples_leaf=1,\n","    min_weight_fraction_leaf=0.0,\n","    max_features='auto',\n","    max_leaf_nodes=None,\n","    min_impurity_decrease=0.0,\n","    min_impurity_split=None,\n","    bootstrap=True,\n","    oob_score=False,\n","    n_jobs=None,\n","    random_state=None,\n","    verbose=0,\n","    warm_start=False,\n","    class_weight=None,\n","    ccp_alpha=0.0,\n","    max_samples=None)\n","    ```\n","\n","We will impliment GridSearchCV and RandomizedSearchCV for hyper parameter tuning and will find out best hyper parameter."],"metadata":{"id":"hVlgAIj8Yqjj"}},{"cell_type":"markdown","source":["**Grid Search CV**"],"metadata":{"id":"t-pKAyv_Z7zm"}},{"cell_type":"code","source":["grid_param = {\n","    'n_estimators': [100, 300, 500, 800, 1000],\n","    'criterion': ['gini', 'entropy'],\n","    'bootstrap': [True, False]\n","}"],"metadata":{"id":"ZcM0gEtlYp-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","import time\n","\n","gd_sr = GridSearchCV(estimator=classifier,\n","                     param_grid=grid_param,\n","                     scoring='accuracy',\n","                     cv=5,\n","                     n_jobs=-1)"],"metadata":{"id":"eQEEhV1VYkVV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = time.time()\n","gd_sr.fit(X_train, y_train)\n","print('It Got Executed in', time.time() - start, 'Seconds')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JlgG44s4Zc7b","executionInfo":{"status":"ok","timestamp":1669541207940,"user_tz":-330,"elapsed":206120,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"22d7a360-496a-4e97-afb1-f48411d25e07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["It Got Executed in 205.5446801185608 Seconds\n"]}]},{"cell_type":"code","source":["best_parameters = gd_sr.best_params_\n","print(best_parameters)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yBxl12soZiwt","executionInfo":{"status":"ok","timestamp":1669541221378,"user_tz":-330,"elapsed":554,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"dd554bc5-320b-45e5-fa0a-26f2babf6282"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'bootstrap': True, 'criterion': 'gini', 'n_estimators': 800}\n"]}]},{"cell_type":"code","source":["best_result = gd_sr.best_score_\n","print(best_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DLd6lHTKZnQ1","executionInfo":{"status":"ok","timestamp":1669541224691,"user_tz":-330,"elapsed":565,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"2d9b9b24-9dd9-476a-9679-857eaeb4f073"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6630422794117647\n"]}]},{"cell_type":"code","source":["# Use best parameters\n","\n","clf = RandomForestClassifier(bootstrap = True, criterion = 'gini', n_estimators = 800)\n","clf.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jKIdcQ6OZpwA","executionInfo":{"status":"ok","timestamp":1669541241656,"user_tz":-330,"elapsed":3164,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"c7cdc660-bde7-4d31-f252-d017162d88d6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestClassifier(n_estimators=800)"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["ypred = clf.predict(X_test)"],"metadata":{"id":"6Bs_u8PTZvF2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","print(accuracy_score(y_test,ypred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ICq15k_5ac0q","executionInfo":{"status":"ok","timestamp":1669541374370,"user_tz":-330,"elapsed":11,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"1d5316fd-16bd-4d70-a632-3b92c529a89d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.721875\n"]}]},{"cell_type":"markdown","source":["**Randomized Search CV**"],"metadata":{"id":"IbJ3RajuaG1z"}},{"cell_type":"code","source":["rf_params={'max_depth':[3,5,10],\n","              'max_features':(1,2,3,4,5,6),\n","               'criterion':['gini','entropy'],\n","               'bootstrap':[True,False],\n","               'min_samples_leaf':(1,2,3,5,7,8,9,10)\n","              }"],"metadata":{"id":"3TtmpCN7aFn-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import RandomizedSearchCV\n","gd_sr = RandomizedSearchCV(classifier, rf_params, cv = 5, scoring = \"accuracy\", random_state = 0, refit = False)"],"metadata":{"id":"JbEVBYwrZyrO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = time.time()\n","search = gd_sr.fit(X_train, y_train)\n","print('It Got Executed in', time.time() - start, 'Seconds')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZubLLRJbbRsD","executionInfo":{"status":"ok","timestamp":1669541494899,"user_tz":-330,"elapsed":16134,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"a11288ff-b258-4a13-c302-ecbf2c80be81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["It Got Executed in 16.105156421661377 Seconds\n"]}]},{"cell_type":"code","source":["search.best_params_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uvRfW4PFbRbS","executionInfo":{"status":"ok","timestamp":1669541498839,"user_tz":-330,"elapsed":563,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"166da336-251e-4977-e292-99ba8da5c3b2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'min_samples_leaf': 1,\n"," 'max_features': 1,\n"," 'max_depth': 10,\n"," 'criterion': 'gini',\n"," 'bootstrap': True}"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["best_result = search.best_score_\n","print(best_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TeT0loCCbay0","executionInfo":{"status":"ok","timestamp":1669541517948,"user_tz":-330,"elapsed":8,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"c7393fdf-864e-4159-c33f-79e737e6fc29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6466053921568627\n"]}]},{"cell_type":"code","source":["# Use best parameters\n","\n","clf = RandomForestClassifier(min_samples_leaf= 1,\n"," max_features= 1,\n"," max_depth= 10,\n"," criterion='gini',\n"," bootstrap= True)\n","clf.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vFcO759Ubane","executionInfo":{"status":"ok","timestamp":1669541659764,"user_tz":-330,"elapsed":545,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"0871b772-b51a-4a56-977d-bc0f4febc80f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestClassifier(max_depth=10, max_features=1)"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["ypred = clf.predict(X_test)"],"metadata":{"id":"MaERmA_cbaja"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(accuracy_score(y_test,ypred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gfiH1lubaWn","executionInfo":{"status":"ok","timestamp":1669541690827,"user_tz":-330,"elapsed":6,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"2f02d160-3276-4e2e-b8cc-134f7d14069d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.696875\n"]}]},{"cell_type":"markdown","source":["# L1 and L2 Regularization\n","\n","In supervised machine learning, the ML models get trained training data and there are the possibilities that the model performs accurately on training data but fails to perform well on test data and also produces high error due to several factors such as collinearity, bias-variance impact and over modeling on train data.\n","\n"," \n","\n","For example, when the model learns signals as well as noises in the training data but couldn’t perform appropriately on new data upon which the model wasn’t trained, the condition/problem of overfitting takes place.  \n","\n"," \n","\n","Overfitting simply states that there is low error with respect to training dataset, and high error with respect to test datasets. \n","\n"," \n","\n","Various methods can be adopted, for avoiding overfitting of models on training data, such as cross-validation sampling, reducing number of features, pruning, regularization and many more.\n","\n"," \n","\n","As our focus today on overfitting and regularization and its respective techniques, we start simply by learning the concept of regularization and its significant aspect for overfitting. \n","\n","![](https://i.imgur.com/ZHhW06v.png)\n"," \n","**Bias-Variance Tradeoff**\n","\n"," \n","\n","In order to understand how the deviation of the function is varied, bias and variance can be adopted. \n","\n","**Bias** is the measurement of deviation or error from the real value of function, **variance** is the measurement of deviation in the response variable function while estimating it over a different training sample of the dataset.\n","\n"," \n","\n","Therefore, for a generalized data model, we must keep **bias possibly low** while modelling that leads to **high accuracy.** And, one should not obtain greatly varied results from output, therefore, **low variance** is recommended for **a model to perform good**.\n","\n"," \n","\n","The underlying association between bias and variance is closely related to the overfitting, underfitting and capacity in machine learning such that while calculating the generalization error (where bias and variance are crucial elements) increase in the model capacity can lead to increase in variance and decrease in bias.\n","\n"," \n","\n","The trade-off is the tension amid error introduced by the bias and the variance.\n","\n","The image is showing the bias-variance tradeoff in the form of a function of the model complexity.\n","\n","**Bias-Variance Tradeoff** \n","\n","From the graph, it can be observed that;\n","\n"," \n","\n","- While reducing bias, the model fits exactly well on a particular sample of training data, and is unable to find the basic patterns in the dataset that it has never trained. So the model can have deviated outcomes while trained on another sample, and hence produce high variance.\n","\n","- Similarly, if willing to keep minor deviation or low variance when distinct samples datasets are used, then the model would not fit exactly on data points that lead to high bias.\n","\n"," \n","\n","See the graph below, where the conditions of underfitting, exact fit, and overfitting can be observed.\n","\n","\n","Below are the examples (specific algorithms) that shows the bias variance trade-off configuration;\n","\n"," \n","\n","1. The support vector machine algorithm has low bias and high variance, but the trade off may be altered by escalating the cost (C) parameter that can change the quantity of violation of the allowed margin in the training data which decreases the variance and increases the bias.\n","\n","2. The k-nearest neighbor algorithm has low bias and high variance, here the trade-off can be modified through extending the k-value that further increases the number of neighbors contributing in the predictions which consequently increases the bias of the model.\n","\n"," \n","\n","Now, what’s more? We need to focus here that the while modeling the data, a situation of low bias and high variance is termed as overfitting such that the model fits certainly well with high accuracy on available data and when it sees new data it fails to predict accurately that yield high test error.\n","\n"," \n","\n","It often happens when the data has several numbers of features, and the model takes the contribution of all estimated coefficients into consideration and attempts to overestimate the actual value.\n","\n"," \n","\n","In contrast to this, the significant fact is only few features are important in the dataset and impact the prediction.\n","\n"," \n","\n","Therefore, if the insignificant features are huge in number, they can add value to the function in training data, but when the new data comes up that are no connections with these features, the predictions are misinterpreted.\n","\n"," \n","\n","So it becomes very important to confine the features to minimizing the plausibility of overfitting while modeling, and hence the process of regularization is preferred. \n","\n"," \n","\n"," \n","## What is Regularization\n","\n"," \n","\n","In regression analysis, the features are estimated using coefficients while modelling. Also, if the estimates can be restricted, or shrinked or regularized towards zero, then the impact of insignificant features might be reduced and would prevent models from high variance with a stable fit. \n","\n"," \n","\n","**Regularization is the most used technique to penalize complex models in machine learning, it is deployed for reducing overfitting (or, contracting generalization errors) by putting network weights small. Also, it enhances the performance of models for new inputs.\n"," \n","\n","In simple words, **it avoids overfitting by panelizing the regression coefficients of high value.**\n","\n","More specifically, It decreases the parameters and shrinks (simplifies) the model. This more streamlined model will aptly perform more efficiently while making predictions.\n","\n"," \n","\n","Since, it makes the magnitude to weighted values low in a model, regularization technique is also referred to as weight decay.\n","\n"," \n","\n","Moreover, Regularization appends penalties to more complex models and arranges potential models from slightest overfit to greatest. Regularization assumes that least weights may produce simpler models and hence assist in avoiding overfitting.\n","\n"," \n","\n","The model with the least overfitting score is accounted as the preferred choice for prediction. \n","\n"," \n","\n","In general, regularization is adopted universally as simple data models generalize better and are less prone to overfitting. Examples of regularization, included;\n","\n"," \n","\n","- K-means: Restricting the segments for avoiding redundant groups.\n","\n","- Neural networks: Confining the complexity (weights) of a model.\n","\n","- Random Forest: Reducing the depth of tree and branches (new features)\n","\n"," \n","\n","There are various regularization techniques, some well-known techniques are L1, L2 and dropout regularization, however, L1 and L2 regularization is our main course of interest.\n","\n"," \n","**Regularization Term** \n","\n","Both L1 and L2 can add a penalty to the cost depending upon the model complexity, so at the place of computing the cost by using a loss function, there will be an auxiliary component, known as regularization terms, added in order to panelizing complex models. \n","\n","By adding regularization term, the value of weights matrices reduces by assuming that a neural network having less weights makes simpler models. And hence, it reduces the overfitting to a certain level.\n","\n"," \n","**Penalty Terms**\n","\n","Through biasing data points towards specific values such as very small values to zero, Regularization achieves this biasing by adding a tuning parameter to strengthen those data points. Such as;\n","\n","1. **L1 regularization:** It adds an L1 penalty that is equal to the absolute value of the magnitude of coefficient, or simply restricting the size of coefficients. For example, Lasso regression implements this method. \n","\n","2. **L2 Regularization:** It adds an L2 penalty which is equal to the square of the magnitude of coefficients. For example, Ridge regression and SVM implement this method.\n","\n","3. **Elastic Net:** When L1 and L2 regularization combine together, it becomes the elastic net method, it adds a hyperparameter.\n","\n","\n"," \n","**What is L1 Regularization?**\n","\n"," L1 regularization is the preferred choice when having a high number of features as it provides sparse solutions. Even, we obtain the computational advantage because features with zero coefficients can be avoided.\n","\n","The regression model that uses **L1 regularization technique is called Lasso Regression.**\n","\n"," \n","**Mathematical Formula for L1 regularization**\n","\n","For instance, we define the simple linear regression model Y with an independent variable to understand how L1 regularization works.\n","\n","For this model, W and b represents **“weight”** and **“bias”** respectively, such as\n","\n"," \n","\n","$ W= w_1, w_2, w_3, ......... w_n $\n","\n","And,\n","\n","$b=b_1, b_2, b_3, ......... b_n$  \n","\n","And Ŷ is the predicted result such that\n","\n","$Ŷ= w_1 x_1 +w_2 x_2 +......+w_n x_n + b$\n","\n","The below function calculates an error without the regularization function\n","\n","$Loss= Error (Y, Ŷ)$\n","\n","And function that can calculate the error with L1 regularization function,\n","\n","displaying the formula of loss function with L1 regularization\n","\n","![](https://i.imgur.com/hcqfV6H.jpg)\n","\n","Where 𝝺 is called the regularization parameter and 𝝺> 0 is manually tuned. Also, 𝝺=0 then the above loss function acts as Ordinary Least Square where the high range value push the coefficients (weights) 0 and hence make it underfits.\n","\n"," \n","\n","Now |w| is only differentiable everywhere except when w=0 as shown below;\n","\n","**Differentiating weight (w)**\n","\n","![](https://i.imgur.com/wm7j3U2.jpg)\n","\n","Substituting the formula of Gradient Descent optimizer for calculating new weights;\n","\n","**Substituting gradient descent optimizer formula to calculate new weight**\n","\n","![](https://i.imgur.com/WUKWWZy.jpg)\n","\n","Putting the L1 formula in the above equation;\n","\n","![](https://i.imgur.com/NGsXuVc.jpg)\n"," \n","\n","**Displaying the new weight formula equation with regularization parameter.**              \n","\n","              \n","\n","From the above formula, we can say that;\n","\n"," \n","\n","- When w is positive, the regularization parameter (λ > 0) will make w to be least positive, by deducting λ from w.\n","\n","- When w is negative, the regularization parameter (λ < 0) will make w to be little negative, by summing λ to w.\n","\n","  \n","**What is L2 regularization?**\n","\n"," \n","\n","**L2 regularization can deal with the multicollinearity (independent variables are highly correlated) problems through constricting the coefficient and by keeping all the variables. **\n","\n"," \n","\n","L2 regression can be used to estimate the significance of predictors and based on that it can penalize the insignificant predictors.\n","\n"," \n","\n","**A regression model that uses L2 regularization techniques is called Ridge Regression.**\n","\n"," \n","**Mathematical Formula for L2 regularization**\n","\n"," \n","\n","For instance, we define the simple linear regression model Y with an independent variable to understand how L2 regularization works.\n","\n"," \n","\n","For this model, W and b represents **“weight”** and **“bias”** respectively, such as\n","\n","$ W= w_1, w_2, w_3, ......... w_n $\n","\n","And,\n","\n","$b=b_1, b_2, b_3, ......... b_n$  \n","\n","And Ŷ is the predicted result such that\n","\n","$Ŷ= w_1 x_1 +w_2 x_2 +......+w_n x_n + b$\n","\n","\n","The below function calculates an error without the regularization function\n","\n","$Loss= Error (Y, Ŷ)$\n","\n"," \n","\n","And function that can calculate the error with L2 regularization function,\n","\n","**Displaying the formula of loss function with L2 regularization term.**\n","\n","![](https://i.imgur.com/TuddVX2.jpg)\n","\n","Here, 𝝺 is known as Regularization parameter, also if the lambda is zero, this again would act as OLS, and if lambda is extremely large, it leads to adding huge weights and yield as underfitting. \n","\n"," \n","\n","Substituting the formula of Gradient Descent optimizer for calculating new weights;\n","\n","\n","\n","**Displaying the formula of new weight with L2 regularization term.**\n","\n","Putting the L2 formula in the above equation;\n","\n"," ![](https://i.imgur.com/ncp1ILr.jpg)\n","\n","**Showing the new weight equations with L2 regularization term.** \n","\n"," \n","\n","**L2 vs L1 Regularization**\n","\n"," \n","\n","It is often observed that people get confused in selecting the suitable regularization approach to avoid overfitting while training a machine learning model. \n","\n"," \n","\n","Among many regularization techniques, such as L2 and L1 regularization, dropout, data augmentation, and early stopping, we will learn here intuitive differences between L1 and L2 regularization.  \n","\n"," \n","\n","- Where L1 regularization attempts to estimate the median of data, L2 regularization makes estimation for the mean of the data in order to evade overfitting.\n","\n"," - Through including the absolute value of weight parameters, L1 regularization can add the penalty term in cost function. On the other hand, L2 regularization appends the squared value of weights in the cost function.\n","\n","- As defined, sparsity is the characteristic of holding highly significant coefficients, either very close to zero or not very close to zero, where in general coefficients approaching zero would be eliminated later. \n","\n","- And the feature selection is the in-depth of sparsity, i.e. in place of confining coefficients nearby to zero, feature selection is brought them exactly to zero, and hence expel certain features from the data model. \n","\n"," \n","\n","In this context, \n","\n","- L1 regularization can be helpful in features selection by eradicating the unimportant features, whereas, L2 regularization is not recommended for feature selection.\n","\n"," \n","- L2 has a solution in closed form as it’s a square of a weight, on the other side, L1 doesn’t have a closed form solution since it includes an absolute value and it is a non-differentiable function. \n","\n"," \n","\n","Due to this reason, L1 regularization is relatively more expensive in computation, it can’t be solved in the context of matrix measurement and heavily relies on approximations. \n","\n"," \n","\n","L2 regularization is likely to be more accurate in all the circumstances, however, at a much higher level of computational costs.\n","\n"," \n","\n","In order to prevent overfitting, regularization is most-approaches mathematical technique, it achieves this by panelizing the complex ML models via adding regularization terms to the loss function/cost function of the model. \n","\n"," \n","\n","- L1 regularization gives output in binary weights from 0 to 1 for the model’s features and is adopted for decreasing the number of features in a huge dimensional dataset. \n","\n","- L2 regularization disperse the error terms in all the weights that leads to more accurate customized final models.\n"],"metadata":{"id":"6P4tJwtljJOw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jg2QuxBxNc1t"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from statistics import mean\n"]},{"cell_type":"code","source":["url='https://raw.githubusercontent.com/ubaid-shah/datasets/main/kc_house_data.csv'\n","\n","# Loading the data into a Pandas DataFrame\n","data = pd.read_csv(url)\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"id":"gt8_T7SCfom0","executionInfo":{"status":"ok","timestamp":1669542755460,"user_tz":-330,"elapsed":2849,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"02e89c90-8ffa-4f6d-ad97-64c3a1f28bdb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["           id             date     price  bedrooms  bathrooms  sqft_living  \\\n","0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n","1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n","2  5631500400  20150225T000000  180000.0         2       1.00          770   \n","3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n","4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n","\n","   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n","0      5650     1.0           0     0  ...      7      1180.0              0   \n","1      7242     2.0           0     0  ...      7      2170.0            400   \n","2     10000     1.0           0     0  ...      6       770.0              0   \n","3      5000     1.0           0     0  ...      7      1050.0            910   \n","4      8080     1.0           0     0  ...      8      1680.0              0   \n","\n","   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n","0      1955             0    98178  47.5112 -122.257           1340   \n","1      1951          1991    98125  47.7210 -122.319           1690   \n","2      1933             0    98028  47.7379 -122.233           2720   \n","3      1965             0    98136  47.5208 -122.393           1360   \n","4      1987             0    98074  47.6168 -122.045           1800   \n","\n","   sqft_lot15  \n","0        5650  \n","1        7639  \n","2        8062  \n","3        5000  \n","4        7503  \n","\n","[5 rows x 21 columns]"],"text/html":["\n","  <div id=\"df-1f852473-7cc5-474f-a426-629e15868f18\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>date</th>\n","      <th>price</th>\n","      <th>bedrooms</th>\n","      <th>bathrooms</th>\n","      <th>sqft_living</th>\n","      <th>sqft_lot</th>\n","      <th>floors</th>\n","      <th>waterfront</th>\n","      <th>view</th>\n","      <th>...</th>\n","      <th>grade</th>\n","      <th>sqft_above</th>\n","      <th>sqft_basement</th>\n","      <th>yr_built</th>\n","      <th>yr_renovated</th>\n","      <th>zipcode</th>\n","      <th>lat</th>\n","      <th>long</th>\n","      <th>sqft_living15</th>\n","      <th>sqft_lot15</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7129300520</td>\n","      <td>20141013T000000</td>\n","      <td>221900.0</td>\n","      <td>3</td>\n","      <td>1.00</td>\n","      <td>1180</td>\n","      <td>5650</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>7</td>\n","      <td>1180.0</td>\n","      <td>0</td>\n","      <td>1955</td>\n","      <td>0</td>\n","      <td>98178</td>\n","      <td>47.5112</td>\n","      <td>-122.257</td>\n","      <td>1340</td>\n","      <td>5650</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>6414100192</td>\n","      <td>20141209T000000</td>\n","      <td>538000.0</td>\n","      <td>3</td>\n","      <td>2.25</td>\n","      <td>2570</td>\n","      <td>7242</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>7</td>\n","      <td>2170.0</td>\n","      <td>400</td>\n","      <td>1951</td>\n","      <td>1991</td>\n","      <td>98125</td>\n","      <td>47.7210</td>\n","      <td>-122.319</td>\n","      <td>1690</td>\n","      <td>7639</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5631500400</td>\n","      <td>20150225T000000</td>\n","      <td>180000.0</td>\n","      <td>2</td>\n","      <td>1.00</td>\n","      <td>770</td>\n","      <td>10000</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>6</td>\n","      <td>770.0</td>\n","      <td>0</td>\n","      <td>1933</td>\n","      <td>0</td>\n","      <td>98028</td>\n","      <td>47.7379</td>\n","      <td>-122.233</td>\n","      <td>2720</td>\n","      <td>8062</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2487200875</td>\n","      <td>20141209T000000</td>\n","      <td>604000.0</td>\n","      <td>4</td>\n","      <td>3.00</td>\n","      <td>1960</td>\n","      <td>5000</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>7</td>\n","      <td>1050.0</td>\n","      <td>910</td>\n","      <td>1965</td>\n","      <td>0</td>\n","      <td>98136</td>\n","      <td>47.5208</td>\n","      <td>-122.393</td>\n","      <td>1360</td>\n","      <td>5000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1954400510</td>\n","      <td>20150218T000000</td>\n","      <td>510000.0</td>\n","      <td>3</td>\n","      <td>2.00</td>\n","      <td>1680</td>\n","      <td>8080</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>8</td>\n","      <td>1680.0</td>\n","      <td>0</td>\n","      <td>1987</td>\n","      <td>0</td>\n","      <td>98074</td>\n","      <td>47.6168</td>\n","      <td>-122.045</td>\n","      <td>1800</td>\n","      <td>7503</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 21 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f852473-7cc5-474f-a426-629e15868f18')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1f852473-7cc5-474f-a426-629e15868f18 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1f852473-7cc5-474f-a426-629e15868f18');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["# Dropping the numerically non-sensical variables\n","dropColumns = ['id', 'date', 'zipcode']\n","data = data.drop(dropColumns, axis = 1)\n"],"metadata":{"id":"RML8nwzLgNGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["100*data.isna().sum()/data.shape[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kp-sAFFWgF_g","executionInfo":{"status":"ok","timestamp":1669542883587,"user_tz":-330,"elapsed":10,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"aa46fcbe-c1d0-43d0-ca8b-0b8356c778fd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["price            0.0\n","bedrooms         0.0\n","bathrooms        0.0\n","sqft_living      0.0\n","sqft_lot         0.0\n","floors           0.0\n","waterfront       0.0\n","view             0.0\n","condition        0.0\n","grade            0.0\n","sqft_above       0.0\n","sqft_basement    0.0\n","yr_built         0.0\n","yr_renovated     0.0\n","lat              0.0\n","long             0.0\n","sqft_living15    0.0\n","sqft_lot15       0.0\n","dtype: float64"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["data.dropna(inplace=True)"],"metadata":{"id":"DD4otAZ-gYD_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Separating the dependent and independent variables\n","y = data['price']\n","X = data.drop('price', axis = 1)\n","\n","# Dividing the data into training and testing set\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"],"metadata":{"id":"GDRWLQ-CgpVz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Building and fitting the Linear Regression model\n","linearModel = LinearRegression()\n","linearModel.fit(X_train, y_train)\n","\n","# Evaluating the Linear Regression model\n","print(linearModel.score(X_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ulaLL_2ngsip","executionInfo":{"status":"ok","timestamp":1669542918784,"user_tz":-330,"elapsed":7,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"a49a5a18-1875-49d0-9302-56ddd94db6a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6795042992464042\n"]}]},{"cell_type":"code","source":["# List to maintain the different cross-validation scores\n","cross_val_scores_ridge = []\n","\n","# List to maintain the different values of alpha\n","alpha = []\n","\n","# Loop to compute the different values of cross-validation scores\n","for i in range(1, 9):\n","\tridgeModel = Ridge(alpha = i * 0.25)\n","\tridgeModel.fit(X_train, y_train)\n","\tscores = cross_val_score(ridgeModel, X, y, cv = 10)\n","\tavg_cross_val_score = mean(scores)*100\n","\tcross_val_scores_ridge.append(avg_cross_val_score)\n","\talpha.append(i * 0.25)\n","\n","# Loop to print the different values of cross-validation scores\n","for i in range(0, len(alpha)):\n","\tprint(str(alpha[i])+' : '+str(cross_val_scores_ridge[i]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jaAmnoCugyL_","executionInfo":{"status":"ok","timestamp":1669542946166,"user_tz":-330,"elapsed":1436,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"c4ab72de-5885-4fc8-a922-fefbc93f1899"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.25 : 69.0901583767101\n","0.5 : 69.09033468066403\n","0.75 : 69.09049125083845\n","1.0 : 69.0906282899927\n","1.25 : 69.09074599856248\n","1.5 : 69.09084457469035\n","1.75 : 69.09092421425656\n","2.0 : 69.09098511090882\n"]}]},{"cell_type":"markdown","source":["From the above output, we can conclude that the best value of alpha for the data is 2."],"metadata":{"id":"fHrghUVshF6k"}},{"cell_type":"code","source":["# Building and fitting the Ridge Regression model\n","ridgeModelChosen = Ridge(alpha = 2)\n","ridgeModelChosen.fit(X_train, y_train)\n","\n","# Evaluating the Ridge Regression model\n","print(ridgeModelChosen.score(X_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fBTL-vdFhl1m","executionInfo":{"status":"ok","timestamp":1669543147084,"user_tz":-330,"elapsed":554,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"7d96f39e-d667-4e0b-9a6d-149d311c2188"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6792701640742986\n"]}]},{"cell_type":"code","source":["# List to maintain the cross-validation scores\n","cross_val_scores_lasso = []\n","\n","# List to maintain the different values of Lambda\n","Lambda = []\n","\n","# Loop to compute the cross-validation scores\n","for i in range(1, 9):\n","\tlassoModel = Lasso(alpha = i * 0.25, tol = 0.0925)\n","\tlassoModel.fit(X_train, y_train)\n","\tscores = cross_val_score(lassoModel, X, y, cv = 10)\n","\tavg_cross_val_score = mean(scores)*100\n","\tcross_val_scores_lasso.append(avg_cross_val_score)\n","\tLambda.append(i * 0.25)\n","\n","# Loop to print the different values of cross-validation scores\n","for i in range(0, len(alpha)):\n","\tprint(str(alpha[i])+' : '+str(cross_val_scores_lasso[i]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GMVp3BO3g5oE","executionInfo":{"status":"ok","timestamp":1669543022188,"user_tz":-330,"elapsed":4627,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"6163f449-6b64-4580-ad13-5415788b30fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.25 : 69.08996762461632\n","0.5 : 69.08997309354581\n","0.75 : 69.08997853389319\n","1.0 : 69.08998394768221\n","1.25 : 69.0899893270766\n","1.5 : 69.08999467945569\n","1.75 : 69.09000001882953\n","2.0 : 69.09000531192423\n"]}]},{"cell_type":"markdown","source":["From the above output, we can conclude that the best value of lambda is 2."],"metadata":{"id":"re0MV3gdhRWY"}},{"cell_type":"code","source":["# Building and fitting the Lasso Regression Model\n","lassoModelChosen = Lasso(alpha = 2, tol = 0.0925)\n","lassoModelChosen.fit(X_train, y_train)\n","\n","# Evaluating the Lasso Regression model\n","print(lassoModelChosen.score(X_test, y_test))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bLltFzv9hLfT","executionInfo":{"status":"ok","timestamp":1669543054975,"user_tz":-330,"elapsed":588,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"5576dede-4765-4e05-e205-ed031f6ed41b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6794971837277676\n"]}]},{"cell_type":"code","source":["# Building the two lists for visualization\n","models = ['Linear Regression', 'Ridge Regression', 'Lasso Regression']\n","scores = [linearModel.score(X_test, y_test),\n","\t\tridgeModelChosen.score(X_test, y_test),\n","\t\tlassoModelChosen.score(X_test, y_test)]\n","\n","# Building the dictionary to compare the scores\n","mapping = {}\n","mapping['Linear Regression'] = linearModel.score(X_test, y_test)\n","mapping['Ridge Regression'] = ridgeModelChosen.score(X_test, y_test)\n","mapping['Lasso Regression'] = lassoModelChosen.score(X_test, y_test)\n","\n","# Printing the scores for different models\n","for key, val in mapping.items():\n","\tprint(str(key)+' : '+str(val))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nGz13aQFhUbc","executionInfo":{"status":"ok","timestamp":1669543153027,"user_tz":-330,"elapsed":591,"user":{"displayName":"Ubaid Shah (Ghazal)","userId":"14042789612785146189"}},"outputId":"cfc424ca-d0a2-4aa1-83ae-07e15fe96c7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Linear Regression : 0.6795042992464042\n","Ridge Regression : 0.6792701640742986\n","Lasso Regression : 0.6794971837277676\n"]}]}]}