{"cells":[{"cell_type":"markdown","metadata":{"id":"KX32xsvI0MUk"},"source":["# Import software libraries and load the datasets"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cGZ5DbuX2uTr","executionInfo":{"status":"ok","timestamp":1720199118436,"user_tz":-330,"elapsed":28019,"user":{"displayName":"Krishna Dwivedi","userId":"08326203327563828587"}},"outputId":"8077b177-7cd2-42db-829c-4e4e48a2ad18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install visualizenn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26tBs3Pa3JUN","executionInfo":{"status":"ok","timestamp":1720352808323,"user_tz":-330,"elapsed":3554,"user":{"displayName":"Krishna Dwivedi","userId":"08326203327563828587"}},"outputId":"ce83e82b-2478-444f-fe4c-dcd3bfb22765"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement visualizenn (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for visualizenn\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":401},"id":"oePb6OvW0MUs","executionInfo":{"status":"error","timestamp":1720199264783,"user_tz":-330,"elapsed":716,"user":{"displayName":"Krishna Dwivedi","userId":"08326203327563828587"}},"outputId":"3d983094-8545-4198-c626-31c208402d09"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'visualizenn'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-3073b1d1b920>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m                      \u001b[0;31m# Create charts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mvisualizenn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mVisNN\u001b[0m            \u001b[0;31m# Create neural network visualizations. # Fixed import statement to match installed module name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m                  \u001b[0;31m# Calculate training time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'visualizenn'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import sys                             # Read system parameters.\n","import os                              # Interact with the operating system.\n","import numpy as np                     # Work with multi-dimensional arrays and matrices.\n","import pandas as pd                    # Manipulate and analyze data frames.\n","import sklearn                         # Perform feature engineering and machine learning.\n","from sklearn.utils import shuffle\n","import matplotlib                      # Create charts.\n","import matplotlib.pyplot as plt\n","import visualizenn as VisNN            # Create neural network visualizations. # Fixed import statement to match installed module name\n","from time import time                  # Calculate training time.\n","\n","# Summarize software libraries used.\n","print('Libraries used in this project:')\n","print('- NumPy {}'.format(np.__version__))\n","print('- pandas {}'.format(pd.__version__))\n","print('- scikit-learn {}'.format(sklearn.__version__))\n","print('- Matplotlib {}'.format(matplotlib.__version__))\n","print('- Python {}\\n'.format(sys.version))\n","\n","# Load the datasets.\n","DATA_PATH = os.path.join('.', 'occupancy_data')\n","print('Data files in this project:', os.listdir(DATA_PATH))\n","\n","data_file_train_raw = os.path.join(DATA_PATH, 'occupancy_train.csv')\n","data_file_test_raw = os.path.join(DATA_PATH, 'occupancy_test.csv')\n","df_train = pd.read_csv(data_file_train_raw)\n","df_test = pd.read_csv(data_file_test_raw)\n","print('Loaded {} records from {}.'.format(len(df_train), data_file_train_raw))\n","print('Loaded {} records from {}.'.format(len(df_test), data_file_test_raw))"]},{"cell_type":"markdown","metadata":{"id":"GALnlXQr0MUw"},"source":["# Get acquainted with the dataset #"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7GGiUX0t0MUx"},"outputs":[],"source":["# Shuffle the dataset.\n","df_train = shuffle(df_train.copy(), random_state = 765)\n","df_train.reset_index(inplace = True, drop = True)\n","\n","df_test = shuffle(df_test.copy(), random_state = 765)\n","df_test.reset_index(inplace = True, drop = True)\n","\n","print(df_train.info())\n","df_train.head(10)"]},{"cell_type":"markdown","metadata":{"id":"YhtxOfYh0MUy"},"source":["# Examine the distributions of the features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cgPetNiY0MUz"},"outputs":[],"source":["df_train.hist(figsize = (12, 10), grid = False);"]},{"cell_type":"markdown","metadata":{"id":"4ZEWHJaT0MU0"},"source":["# Examine descriptive statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GkBSLnUh0MU1"},"outputs":[],"source":["with pd.option_context('float_format', '{:.3f}'.format):\n","    display(df_train.describe())"]},{"cell_type":"markdown","metadata":{"id":"Xc9e0iYJ0MU2"},"source":["# Split the label from the datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"awaLFsL_0MU3"},"outputs":[],"source":["# Separate training and test sets already exist.\n","\n","# Occupancy is the dependent variable (value to be predicted), so it will be\n","# removed from the training and testing data and put into a separate data frame for labels.\n","\n","label_cols = ['Occupancy']\n","\n","training_cols = ['Date', 'Temperature', 'RelativeHumidity', 'Light', 'CO2', 'HumidityRatio']\n","\n","# Split the training and test datasets and their labels.\n","X_train, y_train = df_train[training_cols].copy(), df_train[label_cols].copy()\n","X_test, y_test = df_test[training_cols].copy(), df_test[label_cols].copy()\n","\n","# Compare number of rows and columns in original data to training and testing sets.\n","print(f'Original set:      {df_train.append(df_test).shape}')\n","print('------------------------------')\n","print(f'Training features: {X_train.shape}')\n","print(f'Testing features:  {X_test.shape}')\n","print(f'Training labels:   {y_train.shape}')\n","print(f'Testing labels:    {y_test.shape}')"]},{"cell_type":"markdown","metadata":{"id":"kY1IEYMN0MU4"},"source":["# Convert the `Date` column to datetime format for processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UC9J61tN0MU4"},"outputs":[],"source":["X_train['Date'] = pd.to_datetime(X_train['Date'])\n","X_test['Date'] = pd.to_datetime(X_test['Date'])\n","\n","X_train.head()"]},{"cell_type":"markdown","metadata":{"id":"vvUA6t1E0MU5"},"source":["# Determine which datetime components have unique values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CvC5H6Mz0MU6"},"outputs":[],"source":["# Extract specific datetime components and retrieve unique values.\n","print('Unique years:   {}'.format(X_train['Date'].dt.year.unique()))\n","print('Unique months:  {}'.format(X_train['Date'].dt.month.unique()))\n","print('Unique days:    {}'.format(X_train['Date'].dt.day.unique()))\n","print('Unique hours:   {}'.format(X_train['Date'].dt.hour.unique()))\n","print('Unique minutes: {}'.format(X_train['Date'].dt.minute.unique()))\n","print('Unique seconds: {}'.format(X_train['Date'].dt.second.unique()))"]},{"cell_type":"markdown","metadata":{"id":"QGZXakzJ0MU6"},"source":["# Split the relevant datetime features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VtGaZB2Q0MU6"},"outputs":[],"source":["def split_dt_features(dataset):\n","\n","    # Retrieve days, hours, and minutes from timestamp.\n","    day = dataset['Date'].dt.day\n","    dataset['Day'] = day.astype('int64')\n","\n","    hour = dataset['Date'].dt.hour\n","    dataset['Hour'] = hour.astype('int64')\n","\n","    minute = dataset['Date'].dt.minute\n","    dataset['Minute'] = minute.astype('int64')\n","\n","    return dataset\n","\n","X_train = split_dt_features(X_train.copy())\n","X_test = split_dt_features(X_test.copy())\n","\n","X_train.head()"]},{"cell_type":"markdown","metadata":{"id":"CH4guPsX0MU7"},"source":["# Drop the original `Date` column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7Oo3ES_0MU7"},"outputs":[],"source":["# Date column been split into multiple columns.\n","print('Columns before drop:\\n\\n{}\\n'.format(list(X_train.columns)))\n","X_train.drop('Date', axis = 1, inplace = True)\n","print('Columns after drop:\\n\\n{}\\n'.format(list(X_train.columns)))\n","\n","X_test.drop('Date', axis = 1, inplace = True)"]},{"cell_type":"markdown","metadata":{"id":"jZhb_iI40MU8"},"source":["# Standardize the features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PmkX7nFr0MU8"},"outputs":[],"source":["from sklearn import preprocessing\n","\n","def standardize(dataset):\n","    df_stand = dataset.copy()\n","    scaler = preprocessing.StandardScaler()\n","\n","    df_stand[dataset.columns] = scaler.fit_transform(df_stand[dataset.columns])\n","\n","    return df_stand\n","\n","X_train = standardize(X_train)\n","X_test = standardize(X_test)\n","\n","print('The features have been standardized.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mtM2rUBK0MU9"},"outputs":[],"source":["with pd.option_context('float_format', '{:.2f}'.format):\n","    display(X_train.describe())"]},{"cell_type":"markdown","metadata":{"id":"RQ2-Cq4j0MU9"},"source":["# Train an MLP model"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"z0n6ETx30MU9"},"outputs":[],"source":["from sklearn.neural_network import MLPClassifier\n","\n","mlp = MLPClassifier(hidden_layer_sizes = (2),\n","                    activation = 'relu',\n","                    solver = 'adam',\n","                    alpha = 0.0001,\n","                    learning_rate_init = 0.001,\n","                    max_iter = 500,\n","                    tol = 1e-4,\n","                    n_iter_no_change = 10,\n","                    verbose = True,\n","                    random_state = 87)\n","\n","start = time()\n","mlp.fit(X_train, np.ravel(y_train))\n","end = time()\n","train_time = (end - start)\n","\n","# Score using the test data.\n","score = mlp.score(X_test, y_test)\n","\n","print('\\nMLP model took {:.2f} seconds to fit.'.format(train_time))\n","print('Accuracy: {:.0f}%'.format(score * 100))"]},{"cell_type":"markdown","metadata":{"id":"I9hFfQnf0MU-"},"source":["# Visualize the loss minimization through gradient descent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eRwB26Pb0MU-"},"outputs":[],"source":["def plot_loss(model):\n","    plt.plot(model.loss_curve_)\n","    plt.title('GD Loss Minimization')\n","    plt.xlabel('Steps')\n","    plt.ylabel('Loss')\n","\n","plot_loss(mlp)"]},{"cell_type":"markdown","metadata":{"id":"dPQmO35Z0MU-"},"source":["# Visualize the neural network architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3FqEPybc0MU-"},"outputs":[],"source":["def nn_diagram(X, y, model, show_weights):\n","    '''Generates structure of network from dataset shapes and hidden layer sizes.'''\n","\n","    nn_struct = np.hstack(([X.shape[1]],\n","                           np.asarray(model.hidden_layer_sizes),\n","                           [y.shape[1]]))\n","\n","    # Only plot weights if specified.\n","    if show_weights == True:\n","        network = VisNN.DrawNN(nn_struct, model.coefs_)\n","    else:\n","        network = VisNN.DrawNN(nn_struct)\n","\n","    network.draw()\n","\n","nn_diagram(X_train, y_train, mlp, False)"]},{"cell_type":"markdown","metadata":{"id":"zDA_s5fb0MU_"},"source":["# Retrieve the neuron weights and bias terms and redraw the network architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cm-Yn0Et0MU_"},"outputs":[],"source":["print('Weights between input layer and hidden layer:')\n","print(mlp.coefs_[0], '\\n')\n","print('Weights between hidden layer and output layer:')\n","print(mlp.coefs_[1], '\\n')\n","print('Bias terms between input layer and hidden layer:')\n","print(mlp.intercepts_[0], '\\n')\n","print('Bias terms between hidden layer and output layer:')\n","print(mlp.intercepts_[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z2R8EeWx0MU_"},"outputs":[],"source":["nn_diagram(X_train, y_train, mlp, True)"]},{"cell_type":"markdown","metadata":{"id":"dSzwV_ZS0MVA"},"source":["# Fit an MLP model using grid search with cross-validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t361qOze0MVA"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","mlp = MLPClassifier(alpha = 0.0001,\n","                    learning_rate_init = 0.001,\n","                    max_iter = 500,\n","                    tol = 1e-4,\n","                    n_iter_no_change = 10,\n","                    random_state = 87)\n","\n","grid = {'hidden_layer_sizes': [(5), (6)],\n","        'activation': ['logistic', 'tanh', 'relu'],\n","        'solver': ['sgd', 'adam']}\n","\n","search = GridSearchCV(mlp, param_grid = grid, scoring = 'accuracy', cv = 5)\n","\n","start = time()\n","search.fit(X_train, np.ravel(y_train))\n","end = time()\n","train_time = (end - start)\n","\n","print('Grid search took {:.2f} seconds to find an optimal fit.'.format(train_time))\n","print(search.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pP9tU_V30MVB"},"outputs":[],"source":["score = search.score(X_test, y_test)\n","\n","print('Accuracy: {:.0f}%'.format(score * 100))"]},{"cell_type":"markdown","metadata":{"id":"0cj-WywD0MVB"},"source":["# Visualize the loss minimization of the optimized model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"83PFVg7t0MVB"},"outputs":[],"source":["plot_loss(search.best_estimator_)"]},{"cell_type":"markdown","metadata":{"id":"vc3SyjOQ0MVC"},"source":["# Visualize the network structure of the optimized model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TXPCb9VM0MVC"},"outputs":[],"source":["nn_diagram(X_train, y_train, search.best_estimator_, True)"]},{"cell_type":"markdown","metadata":{"id":"jTUj-RLs0MVC"},"source":["# Examine the model's predictions on the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bsm0Gzm_0MVC"},"outputs":[],"source":["# Show example predictions with the test data.\n","results = df_test.copy()\n","results['PredictedOccupancy'] = search.predict(X_test)\n","\n","# Clarify ground truth column.\n","results.rename(columns = {'Occupancy': 'ActualOccupancy'}, inplace = True)\n","\n","results.head(10)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}